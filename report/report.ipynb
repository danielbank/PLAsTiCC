{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "Daniel Bank\n",
    "November 3rd, 2018\n",
    "\n",
    "# Overview\n",
    "\n",
    "## Photometric LSST Astronomical Time Series Classification Challenge\n",
    "\n",
    "The Large Synoptic Survey Telescope (LSST), currently under construction in the Atacama desert of Chile, is the most awesome telescope that mankind will have built thus far in history.  It features an 8.4m mirror, 3.2 gigapixel camera, and 6.5m effective aperture [[1](#references)].  Every night, it will gather between 20 and 40 TB of image data of the southern sky.  While sifting through that much data to identify cosmological objects would be impractical for humans, it is a perfect use case for machine learning.\n",
    "\n",
    "In preparation for the massive amounts of observations that will come from the LSST, an open data competition has been launched on [Kaggle](https://www.kaggle.com) called the Photometric LSST Astronomical Time Series Classification Challenge ([PLAsTiCC](https://www.kaggle.com/c/PLAsTiCC-2018)).  The goal of the challenge is to correctly classify time-varying cosmological objects in simulated astronomical time-series data similar to the data that the LSST will generate.\n",
    "\n",
    "I first became aware of this challenge while browsing the Kaggle competitions.  My personal interest in astronomy stems partly from my background as a Physics major but more so from son, Arthur, who loves everything about the night sky.  He has taught me to really appreciate the beauty of the cosmos.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "A\n",
    "\n",
    "## Metrics\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Configure Notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import chain\n",
    "import cesium.featurize as featurize\n",
    "from gatspy.periodic import LombScargleMultiband, LombScargleMultibandFast\n",
    "import pdb\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, Lambda\n",
    "from keras.layers import GRU, Dense, Activation, Dropout, concatenate, Input, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tensorflow.python.client import timeline\n",
    "import re\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "warnings.simplefilter('ignore', RuntimeWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "train_series = pd.read_csv('../input/training_set.csv')\n",
    "train_metadata = pd.read_csv('../input/training_set_metadata.csv')\n",
    "test_metadata = pd.read_csv('../input/test_set_metadata.csv')\n",
    "test_series = pd.read_csv('../input/test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting observation is that classes in the training data set divide evenly into intragalactic and extragalactic categories.  There are no classes that are present in both categories.  Class 99 (the unknown classification) is not present at all in the training dataset.  This last point raises an interesting challenge for our model:  How to predict this mystery class when it is not present at all in the training data?  We will investigate possible solutions to this in the following **Algorithms and Techniques** section.\n",
    "\n",
    "Another challenge is that the training dataset is a small subset of the test dataset and is also a poor representation of it.  This design decision was made to imitate the real-world challenges that astronomers face when trying to classify cosmological objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Galactic vs Extragalactic Code by Kyle Boone\n",
    "# https://www.kaggle.com/kyleboone/naive-benchmark-galactic-vs-extragalactic\n",
    "\n",
    "targets = np.hstack([np.unique(train_metadata['target']), [99]])\n",
    "target_map = {j:i for i, j in enumerate(targets)}\n",
    "target_ids = [target_map[i] for i in train_metadata['target']]\n",
    "train_metadata['target_id'] = target_ids\n",
    "\n",
    "# Intragalactic sources have no redshift! (hostgal_spez == 0)\n",
    "intragalactic_cut = train_metadata['hostgal_specz'] == 0.\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(train_metadata[intragalactic_cut]['target_id'], 15, (0, 15), alpha=0.5, label='Galactic')\n",
    "plt.hist(train_metadata[~intragalactic_cut]['target_id'], 15, (0, 15), alpha=0.5, label='Extragalactic')\n",
    "plt.xticks(np.arange(15)+0.5, targets)\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Counts')\n",
    "plt.xlim(0, 15)\n",
    "plt.title('Intragalactic vs Extragalactic Sources in Training Data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to look at the example passband data for the various classes to get an understanding of what the data looks like and understand some of the distinguishing features between classes.  As there are 14 classes represented in the training dataset, we will only examine two of the more interesting classes: Class 92 and Class 42.  Class 92 are highly periodic while Class 42 exhibit what appear to be \"burst\" events with diminishing activity after that.  The author of this code, [Mithrillion](https://www.kaggle.com/mithrillion), theorizes that Class 92 are regular variable stars and Class 42 are supernovae events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code by Mithrillion\n",
    "# https://www.kaggle.com/mithrillion/all-classes-light-curve-characteristics-updated\n",
    "\n",
    "groups = train_series.groupby(['object_id', 'passband'])\n",
    "times = groups.apply(\n",
    "    lambda block: block['mjd'].values).reset_index().rename(columns={0: 'seq'})\n",
    "flux = groups.apply(\n",
    "    lambda block: block['flux'].values\n",
    ").reset_index().rename(columns={0: 'seq'})\n",
    "err = groups.apply(\n",
    "    lambda block: block['flux_err'].values\n",
    ").reset_index().rename(columns={0: 'seq'})\n",
    "det = groups.apply(\n",
    "    lambda block: block['detected'].astype(bool).values\n",
    ").reset_index().rename(columns={0: 'seq'})\n",
    "times_list = times.groupby('object_id').apply(lambda x: x['seq'].tolist()).tolist()\n",
    "flux_list = flux.groupby('object_id').apply(lambda x: x['seq'].tolist()).tolist()\n",
    "err_list = err.groupby('object_id').apply(lambda x: x['seq'].tolist()).tolist()\n",
    "det_list = det.groupby('object_id').apply(lambda x: x['seq'].tolist()).tolist()\n",
    "\n",
    "def fit_multiband_freq(tup):\n",
    "    idx, group = tup\n",
    "    t, f, e, b = group['mjd'], group['flux'], group['flux_err'], group['passband']\n",
    "    model = LombScargleMultiband(fit_period=True)\n",
    "    model.optimizer.period_range = (0.1, int((group['mjd'].max() - group['mjd'].min()) / 2))\n",
    "    model.fit(t, f, e, b)\n",
    "    return model\n",
    "\n",
    "def get_freq_features(N, subsetting_pos=None):\n",
    "    if subsetting_pos is None:\n",
    "        subset_times_list = times_list\n",
    "        subset_flux_list = flux_list\n",
    "    else:\n",
    "        subset_times_list = [v for i, v in enumerate(times_list) \n",
    "                             if i in set(subsetting_pos)]\n",
    "        subset_flux_list = [v for i, v in enumerate(flux_list) \n",
    "                            if i in set(subsetting_pos)]\n",
    "    feats = featurize.featurize_time_series(times=subset_times_list[:N],\n",
    "                                            values=subset_flux_list[:N],\n",
    "                                            features_to_use=['skew',\n",
    "                                                            'percent_beyond_1_std',\n",
    "                                                            'percent_difference_flux_percentile'\n",
    "                                                            ],\n",
    "                                            scheduler=None)\n",
    "    subset = train_series[train_series['object_id'].isin(\n",
    "        train_metadata['object_id'].iloc[subsetting_pos].iloc[:N])]\n",
    "    models = list(map(fit_multiband_freq, subset.groupby('object_id')))\n",
    "    feats['object_pos'] = subsetting_pos[:N]\n",
    "    feats['freq1_freq'] = [model.best_period for model in models]\n",
    "    return feats, models\n",
    "\n",
    "unique_classes = train_metadata['target'].unique()\n",
    "\n",
    "def get_class_feats(label, N=10):\n",
    "    class_pos = train_metadata[train_metadata['target'] == label].index\n",
    "    class_feats, class_models = get_freq_features(N, class_pos)\n",
    "    return class_feats, class_models\n",
    "\n",
    "def plot_phase_curves(feats, models, use_median_freq=False, hide_undetected=True, N=10):\n",
    "    for i in range(N):\n",
    "        freq = feats.loc[i, 'freq1_freq'].median()\n",
    "        freq_min = feats.loc[i, 'freq1_freq'].min()\n",
    "        freq_std = feats.loc[i, 'freq1_freq'].std()\n",
    "        skew = feats.loc[i, 'skew'].mean()\n",
    "        object_pos = int(feats.loc[i, 'object_pos'][0])\n",
    "        f, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "        sample = train_series[train_series['object_id'] ==\n",
    "                              train_metadata['object_id'].iloc[object_pos]].copy()\n",
    "        colors = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\n",
    "        score = models[i].score(models[i].best_period)\n",
    "        \n",
    "        ax[0].scatter(x=sample['mjd'], \n",
    "                   y=sample['flux'], \n",
    "                   c=[colors[b] for b in sample['passband']],\n",
    "                   s=8, alpha=0.8)\n",
    "        ax[0].vlines(sample['mjd'], \n",
    "                  sample['flux'] - sample['flux_err'],\n",
    "                  sample['flux'] + sample['flux_err'],\n",
    "                  colors=[colors[b] for b in sample['passband']],\n",
    "                  linewidth=1, alpha=0.8)\n",
    "        \n",
    "        sample['phase'] = (sample['mjd'] / models[i].best_period) % 1\n",
    "        ax[1].scatter(x=sample['phase'], \n",
    "                   y=sample['flux'], \n",
    "                   c=[colors[b] for b in sample['passband']],\n",
    "                   s=8, alpha=0.8)\n",
    "        ax[1].vlines(sample['phase'], \n",
    "                  sample['flux'] - sample['flux_err'],\n",
    "                  sample['flux'] + sample['flux_err'],\n",
    "                  colors=[colors[b] for b in sample['passband']],\n",
    "                  linewidth=1, alpha=0.8)\n",
    "        x_range = np.linspace(sample['mjd'].min(), sample['mjd'].max(), 1000)\n",
    "        for band in range(6):\n",
    "            y = models[i].predict(x_range, band)\n",
    "            xs = (x_range / models[i].best_period) % 1\n",
    "            ords = np.argsort(xs)\n",
    "            ax[1].plot(xs[ords], y[ords], c=colors[band], alpha=0.4)\n",
    "        \n",
    "        title = ax[0].get_title()\n",
    "        ax[0].set_title('time')\n",
    "        ax[1].set_title('phase')\n",
    "        f.suptitle(title + f'object: {sample[\"object_id\"].iloc[0]}, '\n",
    "                   f'class: {train_metadata[\"target\"].iloc[object_pos]}\\n'\n",
    "                   f'period: {models[i].best_period: .4}, '\n",
    "                   f'period score: {score: .4}, '\n",
    "                   f'mean skew: {skew:.4}', y=1.1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "feats, models = get_class_feats(92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_phase_curves(feats, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "feats, models = get_class_feats(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_phase_curves(feats, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partition data along intra- extra-galactic\n",
    "- How to handle class 99?  DBScan?  Something else?\n",
    "- Learning model (CNN...)\n",
    "- PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "The PLAsTiCC team has provided a [naive classifier](https://www.kaggle.com/michaelapers/the-plasticc-astronomy-classification-demo) for the challenge.  The naive classifier is designed as follows:\n",
    "\n",
    "1. The raw tabular data is converted into a form suitable for analysis.  Using the `cesium` package for Python, `Timeseries` objects are constructed for each light curve.\n",
    "2. The data is split into a training and test set.\n",
    "3. The dimensionality of the training data is reduced using Principle Component Analysis (PCA).  The result of this step are features which are linear combinations of the `passband` data.  With just eight (8) features, we can account for almost all of the explained variance.\n",
    "4. The reduced dataset is fit with a `RandomForestClassifier`.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The performance of the naive classifier on the test data can be visualized in the following confusion matrix:\n",
    "\n",
    "![Naive Classifier Confusion Matrix](https://github.com/danielbank/PLAsTiCC/blob/master/.github/naive_classifier_confusion_matrix.png?raw=true)\n",
    "\n",
    "In this matrix, we can see the model's predicted labels versus the true labels for the test set.  Ideally, this confusion matrix would be the Identity matrix, with all values along the diagonal being 1 and everything else being 0.  That would mean that the model predicted the correct label 100% of the time.\n",
    "\n",
    "When designing my classifier, I will generate a similar confusion matrix so that my results can be objectively compared to the naive classifier.  My objective is to have an average prediction accuracy per label that is higher than the naive classifier's accuracy of 0.518 (see calculation below):\n",
    "\n",
    "```\n",
    "(1.00 + 0.06 + 0.31 + 0.23 + 0.02 +\n",
    " 0.19 + 0.86 + 0.76 + 0.34 + 0.59 +\n",
    " 0.96 + 0.70 + 0.36 + 0.87) / 14\n",
    "= 0.518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAsTiCC Prediction Pipeline\n",
    "\n",
    "Using [Higepon's CNN based model](https://www.kaggle.com/higepon/keras-cnn-use-time-series-data-as-is) as a starting point, we design our prediction pipeline as follows:\n",
    "\n",
    "1. Data Preprocessing: specific features are scaled using a `StandardScaler`\n",
    "2. Partitioning Intragalactic and Extragalactic Sources\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "3. Both segmented datasets are used to train separate CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The data for this challenge has been provided in a curated form by the PLAsTiCC Team.  As such most data preprocessing techniques, like defaulting missing data or one-hot encoding text categories, are not required for our pipeline.  We do use `sklearn.preprocessing.StandardScaler` to scale features to unit variance which is helpful for CNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_series[['mjd', 'flux', 'flux_err']] = scaler.fit_transform(train_series[['mjd', 'flux', 'flux_err']])\n",
    "train_series = train_series.sort_values(['object_id', 'passband', 'mjd'])\n",
    "\n",
    "test_series[['mjd', 'flux', 'flux_err']] = scaler.fit_transform(test_series[['mjd', 'flux', 'flux_err']])\n",
    "test_series = test_series.sort_values(['object_id', 'passband', 'mjd'])\n",
    "\n",
    "train_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Intragalactic and Extragalactic Sources\n",
    "\n",
    "As we saw in the Data Exploration section, we can narrow number of possible classifications of the training data to 5 out of 14 possibilities for galactic sources and 9 out of 14 possibilities for extragalactic sources.  It makes sense to partition our data along these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the Training Data (Intragalactic vs Extragalactic)\n",
    "\n",
    "intragal_metadata = train_metadata[train_metadata['hostgal_specz'] == 0.]\n",
    "intragal_series = train_series[np.in1d(train_series['object_id'], intragal_metadata['object_id'])]\n",
    "\n",
    "extragal_metadata = train_metadata[train_metadata['hostgal_specz'] != 0.]\n",
    "extragal_series = train_series[np.in1d(train_series['object_id'], extragal_metadata['object_id'])]\n",
    "\n",
    "print('Length of Intragalactic Metadata: ', len(intragal_metadata))\n",
    "print('Length of Extragalactic Metadata: ', len(extragal_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intragal_timeseries = intragal_series.groupby(['object_id', 'passband'])['flux', 'flux_err', 'detected'].apply(lambda df: df.reset_index(drop=True)).unstack()\n",
    "train_intragal_timeseries.fillna(0, inplace=True)\n",
    "train_intragal_timeseries.columns = ['_'.join(map(str,tup)).rstrip('_') for tup in train_intragal_timeseries.columns.values]\n",
    "\n",
    "train_extragal_timeseries = extragal_series.groupby(['object_id', 'passband'])['flux', 'flux_err', 'detected'].apply(lambda df: df.reset_index(drop=True)).unstack()\n",
    "train_extragal_timeseries.fillna(0, inplace=True)\n",
    "train_extragal_timeseries.columns = ['_'.join(map(str,tup)).rstrip('_') for tup in train_extragal_timeseries.columns.values]\n",
    "train_extragal_timeseries.head(7)\n",
    "\n",
    "test_timeseries = test_series.groupby(['object_id', 'passband'])['flux', 'flux_err', 'detected'].apply(lambda df: df.reset_index(drop=True)).unstack()\n",
    "test_timeseries.fillna(0, inplace=True)\n",
    "test_timeseries.columns = ['_'.join(map(str,tup)).rstrip('_') for tup in test_timeseries.columns.values]\n",
    "test_timeseries.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns_intragal = len(train_intragal_timeseries.columns)\n",
    "num_columns_extragal = len(train_extragal_timeseries.columns)\n",
    "print('Number of Columns for Intragalactic Time Series Data: ', num_columns_intragal)\n",
    "print('Number of Columns for Extragalactic Time Series Data: ', num_columns_extragal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_intragal_train = train_intragal_timeseries.values.reshape(-1, 6, num_columns_intragal).transpose(0, 2, 1)\n",
    "X_extragal_train = train_extragal_timeseries.values.reshape(-1, 6, num_columns_extragal).transpose(0, 2, 1)\n",
    "X_test = test_timeseries.values.reshape(-1, 6, num_columns_extragal).transpose(0, 2, 1)\n",
    "X_extragal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intragal_classes = sorted(intragal_metadata.target.unique())\n",
    "class_intragal_map = dict()\n",
    "for i,val in enumerate(intragal_classes):\n",
    "    class_intragal_map[val] = i\n",
    "\n",
    "extragal_classes = sorted(extragal_metadata.target.unique())\n",
    "class_extragal_map = dict()\n",
    "for i,val in enumerate(extragal_classes):\n",
    "    class_extragal_map[val] = i\n",
    "\n",
    "print('Intragalactic Classes Dictionary: ', class_intragal_map)\n",
    "print('Extragalactic Classes Dictionary: ', class_extragal_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intragal_timeseries0 = train_intragal_timeseries.reset_index()\n",
    "train_intragal_timeseries0 = train_intragal_timeseries0[train_intragal_timeseries0.passband == 0]\n",
    "\n",
    "train_extragal_timeseries0 = train_extragal_timeseries.reset_index()\n",
    "train_extragal_timeseries0 = train_extragal_timeseries0[train_extragal_timeseries0.passband == 0]\n",
    "\n",
    "test_timeseries0 = test_timeseries.reset_index()\n",
    "test_timeseries0 = test_timeseries0[test_timeseries0.passband == 0]\n",
    "\n",
    "train_extragal_timeseries0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intragal_metadata = train_intragal_timeseries0.merge(intragal_metadata, on='object_id', how='left')\n",
    "merged_intragal_metadata.fillna(0, inplace=True)\n",
    "\n",
    "merged_extragal_metadata = train_extragal_timeseries0.merge(extragal_metadata, on='object_id', how='left')\n",
    "merged_extragal_metadata.fillna(0, inplace=True)\n",
    "\n",
    "merged_test_metadata = train_test_timeseries0.merge(test_metadata, on='object_id', how='left')\n",
    "merged_test_metadata.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_intragal = merged_intragal_metadata.target\n",
    "classes_intragal = sorted(y_intragal.unique())\n",
    "\n",
    "y_extragal = merged_extragal_metadata.target\n",
    "classes_extragal = sorted(y_extragal.unique())\n",
    "\n",
    "print('Unique Intragalactic Classes: ', classes_intragal)\n",
    "print('Unique Extragalactic Classes: ', classes_extragal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_intragal = merged_intragal_metadata.target\n",
    "target_intragal_map = np.zeros((targets_intragal.shape[0],))\n",
    "target_intragal_map = np.array([class_intragal_map[val] for val in targets_intragal])\n",
    "Y_intragal = to_categorical(target_intragal_map)\n",
    "\n",
    "targets_extragal = merged_extragal_metadata.target\n",
    "target_extragal_map = np.zeros((targets_extragal.shape[0],))\n",
    "target_extragal_map = np.array([class_extragal_map[val] for val in targets_extragal])\n",
    "Y_extragal = to_categorical(target_extragal_map)\n",
    "\n",
    "targets_test = merged_test_metadata.target\n",
    "target_test_map = np.zeros((targets_test.shape[0],))\n",
    "target_test_map = np.array([class_test_map[val] for val in targets_test])\n",
    "Y_test = to_categorical(target_test_map)\n",
    "Y_extragal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Extragalactic CNN Model\n",
    "\n",
    "epochs = 10\n",
    "y_extragal_count = Counter(target_extragal_map)\n",
    "wtable = np.zeros((len(classes_extragal),))\n",
    "for i in range(len(classes_extragal)):\n",
    "    wtable[i] = y_extragal_count[i] / target_extragal_map.shape[0]\n",
    "\n",
    "y_extragal_map = target_extragal_map\n",
    "y_extragal_categorical = Y_extragal\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "start = time.time()\n",
    "clfs_extragal = []\n",
    "oof_extragal_preds = np.zeros((len(X_extragal_train), len(classes_extragal)))\n",
    "\n",
    "model_extragal_file = \"model_extragal.weights\"\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_extragal_map, y_extragal_map)):\n",
    "    checkPoint = ModelCheckpoint(model_extragal_file, monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "\n",
    "    x_extragal_train, y_extragal_train = X_extragal_train[trn_], Y_extragal[trn_]\n",
    "    x_extragal_valid, y_extragal_valid = X_extragal_train[val_], Y_extragal[val_]\n",
    "\n",
    "    model_extragal = build_model(X_extragal_train, classes_extragal)\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    stopping = EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "\n",
    "    model_extragal.compile(loss=mywloss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    history_extragal = model_extragal.fit(x_extragal_train, y_extragal_train,\n",
    "                    validation_data=[x_extragal_valid, y_extragal_valid],\n",
    "                    epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                    shuffle=False,verbose=1,callbacks=[checkPoint, stopping])\n",
    "    plot_loss_acc(history_extragal)\n",
    "\n",
    "    print('Loading Best Model')\n",
    "    model_extragal.load_weights(model_extragal_file)\n",
    "    # Get predicted probabilities for each class\n",
    "    oof_extragal_preds[val_, :] = model_extragal.predict(x_extragal_valid,batch_size=batch_size)\n",
    "    clfs_extragal.append(model_extragal)\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(\"elapsed_time:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Intragalactic CNN Model\n",
    "\n",
    "epochs = 10\n",
    "y_intragal_count = Counter(target_intragal_map)\n",
    "wtable = np.zeros((len(classes_intragal),))\n",
    "for i in range(len(classes_intragal)):\n",
    "    wtable[i] = y_intragal_count[i] / target_intragal_map.shape[0]\n",
    "\n",
    "y_intragal_map = target_intragal_map\n",
    "y_intragal_categorical = Y_intragal\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "start = time.time()\n",
    "clfs_intragal = []\n",
    "oof_intragal_preds = np.zeros((len(X_intragal_train), len(classes_intragal)))\n",
    "\n",
    "model_intragal_file = \"model_intragal.weights\"\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_intragal_map, y_intragal_map)):\n",
    "    checkPoint = ModelCheckpoint(model_intragal_file, monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "\n",
    "    x_intragal_train, y_intragal_train = X_intragal_train[trn_], Y_intragal[trn_]\n",
    "    x_intragal_valid, y_intragal_valid = X_intragal_train[val_], Y_intragal[val_]\n",
    "\n",
    "    model_intragal = build_model(X_intragal_train, classes_intragal)\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    stopping = EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "\n",
    "    model_intragal.compile(loss=mywloss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    history_intragal = model_intragal.fit(x_intragal_train, y_intragal_train,\n",
    "                    validation_data=[x_intragal_valid, y_intragal_valid],\n",
    "                    epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                    shuffle=False,verbose=1,callbacks=[checkPoint, stopping])\n",
    "    plot_loss_acc(history_intragal)\n",
    "\n",
    "    print('Loading Best Model')\n",
    "    model_intragal.load_weights(model_intragal_file)\n",
    "    # Get predicted probabilities for each class\n",
    "    oof_intragal_preds[val_, :] = model_intragal.predict(x_intragal_valid,batch_size=batch_size)\n",
    "    clfs_intragal.append(model_intragal)\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(\"elapsed_time:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest  = np.array(train_metadata['target'][test_ind].tolist())\n",
    "cm = confusion_matrix(Ytest, Ypred, labels=labels)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "annot = np.around(cm, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-Form Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a name=\"reference-1\"></a>[1] - [PLAsTiCC Astronomy Starter Kit](https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
